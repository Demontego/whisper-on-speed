{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demontego/whisper/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import Audio, Dataset\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.cache_implementation = \"static\"\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio(sampling_rate=16_000, mono=True, decode=True)\n",
    "audio_dataset = Dataset.from_dict({\"audio\": [\n",
    "    \"/home/demontego/whisper/tests/test_data/output.wav\",\n",
    "    \"/home/demontego/whisper/tests/test_data/whisper_audio.wav\",\n",
    "    ]}).cast_column(\"audio\", audio)\n",
    "sample = audio_dataset[1]['audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up step: 100%|██████████| 2/2 [00:09<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from tqdm import tqdm\n",
    "\n",
    "for _ in tqdm(range(2), desc=\"Warm-up step\"):\n",
    "    with sdpa_kernel(SDPBackend.MATH):\n",
    "        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256}, return_timestamps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demontego/whisper/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with sdpa_kernel(SDPBackend.MATH):\n",
    "    result = pipe(audio_dataset['audio'].copy(), return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Продукт-менеджер, у него огромный опыт работы с GNI, с разными продуктами, с бизнесом, понимает ограничения, в том числе запросы, поэтому он здесь очень полезен. Наверное, если сделаешь небольшое интро про то, где работал, тоже, чтобы ребятам было комфортно, и потом расскажу о ребятах. Да, да, да. Последний год я работаю над консалтингом в AI. Вот мы со Сбером, с Яндексом, вот с Нитологией и с другими прикольными компаниями как раз повнедряли какие-то разные штуки.',\n",
       " 'chunks': [{'timestamp': (0.32, 5.2),\n",
       "   'text': ' Продукт-менеджер, у него огромный опыт работы с GNI, с разными продуктами, с бизнесом,'},\n",
       "  {'timestamp': (5.3, 9.7),\n",
       "   'text': ' понимает ограничения, в том числе запросы, поэтому он здесь очень полезен.'},\n",
       "  {'timestamp': (10.82, 16.0),\n",
       "   'text': ' Наверное, если сделаешь небольшое интро про то, где работал, тоже, чтобы ребятам было комфортно,'},\n",
       "  {'timestamp': (16.12, 17.28), 'text': ' и потом расскажу о ребятах.'},\n",
       "  {'timestamp': (17.32, 21.56),\n",
       "   'text': ' Да, да, да. Последний год я работаю над консалтингом в AI.'},\n",
       "  {'timestamp': (21.76, 26.86),\n",
       "   'text': ' Вот мы со Сбером, с Яндексом, вот с Нитологией и с другими прикольными компаниями'},\n",
       "  {'timestamp': (26.86, 0.0), 'text': ''},\n",
       "  {'timestamp': (2.04, None),\n",
       "   'text': ' как раз повнедряли какие-то разные штуки.'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up step:   0%|          | 0/30 [00:00<?, ?it/s]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Warm-up step:   3%|▎         | 1/30 [00:08<03:52,  8.03s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Warm-up step:   7%|▋         | 2/30 [00:15<03:38,  7.82s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Warm-up step:  10%|█         | 3/30 [00:23<03:29,  7.76s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Warm-up step:  13%|█▎        | 4/30 [00:31<03:21,  7.77s/it]Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
      "Warm-up step:  17%|█▋        | 5/30 [00:46<03:50,  9.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m30\u001b[39m), desc=\u001b[33m\"\u001b[39m\u001b[33mWarm-up step\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m sdpa_kernel(SDPBackend.MATH):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         result = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/pipelines/automatic_speech_recognition.py:295\u001b[39m, in \u001b[36mAutomaticSpeechRecognitionPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    236\u001b[39m     inputs: Union[np.ndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    237\u001b[39m     **kwargs,\n\u001b[32m    238\u001b[39m ):\n\u001b[32m    239\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[33;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    documentation for more information.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    293\u001b[39m \u001b[33;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1423\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1424\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1426\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[32m   1427\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1428\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/pipelines/pt_utils.py:269\u001b[39m, in \u001b[36mPipelinePackIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/pipelines/base.py:1338\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1337\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/pipelines/automatic_speech_recognition.py:527\u001b[39m, in \u001b[36mAutomaticSpeechRecognitionPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    525\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps == \u001b[33m\"\u001b[39m\u001b[33mword\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type == \u001b[33m\"\u001b[39m\u001b[33mseq2seq_whisper\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2616\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n\u001b[32m   2615\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2616\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2626\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2627\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2628\u001b[39m         batch_size=batch_size,\n\u001b[32m   2629\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2635\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2636\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:4114\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4105\u001b[39m running_sequences, running_beam_scores, running_beam_indices = \u001b[38;5;28mself\u001b[39m._get_running_beams_for_next_iteration(\n\u001b[32m   4106\u001b[39m     topk_log_probs=topk_log_probs,\n\u001b[32m   4107\u001b[39m     topk_running_sequences=topk_running_sequences,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4110\u001b[39m     num_beams=num_beams,\n\u001b[32m   4111\u001b[39m )\n\u001b[32m   4113\u001b[39m \u001b[38;5;66;03m# f. Update the completed beams if a new high score in a finished sequence is found\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4114\u001b[39m sequences, beam_scores, beam_indices, is_sent_finished = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_finished_beams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4115\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopk_running_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk_running_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeam_scores\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopk_log_probs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk_log_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopk_running_beam_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopk_running_beam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_sent_finished\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_sent_finished\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnext_token_hits_stopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_token_hits_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_num_beam_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_num_beam_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4131\u001b[39m \u001b[38;5;66;03m# g. Prepare remaining data for the next iteration, including computing the stopping condition for\u001b[39;00m\n\u001b[32m   4132\u001b[39m \u001b[38;5;66;03m# beam search as a whole (as opposed to individual beams, i.e. `stopping_criteria`)\u001b[39;00m\n\u001b[32m   4133\u001b[39m \n\u001b[32m   4134\u001b[39m \u001b[38;5;66;03m# pluck the cache from the beam indices that will be used in the next iteration\u001b[39;00m\n\u001b[32m   4135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3869\u001b[39m, in \u001b[36mGenerationMixin._update_finished_beams\u001b[39m\u001b[34m(self, sequences, topk_running_sequences, beam_scores, topk_log_probs, beam_indices, topk_running_beam_indices, is_sent_finished, next_token_hits_stopping_criteria, top_num_beam_mask, num_beams, cur_len, decoder_prompt_len, length_penalty, early_stopping)\u001b[39m\n\u001b[32m   3867\u001b[39m merged_is_sent_finished = torch.cat((is_sent_finished, did_top_num_beams_just_finished), dim=\u001b[32m1\u001b[39m)\n\u001b[32m   3868\u001b[39m topk_merged_indices = torch.topk(merged_scores, k=num_beams)[\u001b[32m1\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m3869\u001b[39m sequences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gather_beams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk_merged_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3870\u001b[39m beam_scores = \u001b[38;5;28mself\u001b[39m._gather_beams(merged_scores, topk_merged_indices)\n\u001b[32m   3871\u001b[39m beam_indices = \u001b[38;5;28mself\u001b[39m._gather_beams(merged_beam_indices, topk_merged_indices)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/whisper/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3707\u001b[39m, in \u001b[36mGenerationMixin._gather_beams\u001b[39m\u001b[34m(tensor, beam_indices)\u001b[39m\n\u001b[32m   3705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(beam_indices.shape) < \u001b[38;5;28mlen\u001b[39m(tensor.shape):\n\u001b[32m   3706\u001b[39m     beam_indices = beam_indices.unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3707\u001b[39m gathered_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_along_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m gathered_tensor\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sample2 = audio_dataset['audio']\n",
    "\n",
    "for _ in tqdm(range(30), desc=\"Warm-up step\"):\n",
    "    with sdpa_kernel(SDPBackend.MATH):\n",
    "        result = pipe(sample2.copy(), return_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' я читаю про дипсик и просто как индекс филе до этого не додумались просто нам прикольная прям архитектура что типа на поверхность еще там способ обучения типа он на много раз обсуждался вот почему-то и редко и видео в применении я знаю почему они пошли по этому пути они начали обучать давно очень модель кодинга дипсик кодер у них была лучшая модель вреди всех даже лучше чем чат gpt и они такие надо еще лучше и расширили за пределы кодинга свою модель гениев считаю отчетом вот такое reinforcement learning как-то используется активно или в чем-то да там rl активно используется и там четыре этапа обучения идет вот номер эль там типа решает и первый второй этап он реальный и типа нет модели вознаграждения есть только правильный неправильный ответ вот типа всякие математические задачи логические задачи по химии по биологии по физике вот что-то такое это из науки на финальной стадии у них типа нет на финальной стадии у них есть типа ответ от модели и его сравнивают с истинным насколько он близок вот это и тот ответ который самый близкий там по формуле расписывается чтобы он был наиболее вероятным и дальше пропускают градиент а то есть уже потом обучают лосс далее уже дают функцию пакет ну да блин ну конечно в этих всех темах типа как оценить качество да насколько ответ',\n",
       " 'chunks': [{'timestamp': (0.0, 28.86),\n",
       "   'text': ' я читаю про дипсик и просто как индекс филе до этого не додумались просто'},\n",
       "  {'timestamp': (28.86, 0.0), 'text': ''},\n",
       "  {'timestamp': (9.42, 19.5),\n",
       "   'text': ' нам прикольная прям архитектура что типа на поверхность еще там способ обучения типа он на много раз обсуждался вот почему-то и редко и видео в применении'},\n",
       "  {'timestamp': (24.3, 0.0), 'text': ''},\n",
       "  {'timestamp': (11.4, 17.94),\n",
       "   'text': ' я знаю почему они пошли по этому пути они начали обучать давно очень модель кодинга дипсик кодер у них была лучшая модель вреди всех даже лучше чем чат gpt'},\n",
       "  {'timestamp': (17.94, 26.7),\n",
       "   'text': ' и они такие надо еще лучше и расширили за пределы кодинга свою модель'},\n",
       "  {'timestamp': (26.7, 0.0), 'text': ''},\n",
       "  {'timestamp': (11.04, 20.82),\n",
       "   'text': ' гениев считаю отчетом вот такое reinforcement learning как-то используется активно или в чем-то да там rl активно используется и там четыре этапа обучения идет вот номер эль там типа решает'},\n",
       "  {'timestamp': (24.0, 0.0), 'text': ''},\n",
       "  {'timestamp': (12.72, 17.88),\n",
       "   'text': ' и первый второй этап он реальный и типа нет модели вознаграждения есть только правильный неправильный ответ вот типа всякие математические задачи'},\n",
       "  {'timestamp': (17.88, 26.84),\n",
       "   'text': ' логические задачи по химии по биологии по физике вот что-то такое'},\n",
       "  {'timestamp': (26.84, 0.0), 'text': ''},\n",
       "  {'timestamp': (2.64, 19.44),\n",
       "   'text': ' это из науки на финальной стадии у них типа нет на финальной стадии у них есть типа ответ от модели и его'},\n",
       "  {'timestamp': (19.44, 27.6),\n",
       "   'text': ' сравнивают с истинным насколько он близок вот это и тот ответ который самый близкий там по формуле'},\n",
       "  {'timestamp': (27.6, 0.0), 'text': ''},\n",
       "  {'timestamp': (5.58, 10.5),\n",
       "   'text': ' расписывается чтобы он был наиболее вероятным и дальше пропускают градиент'},\n",
       "  {'timestamp': (17.38, 22.56),\n",
       "   'text': ' а то есть уже потом обучают лосс далее уже дают функцию пакет ну да'},\n",
       "  {'timestamp': (27.86, None),\n",
       "   'text': ' блин ну конечно в этих всех темах типа как оценить качество да насколько ответ'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, s = librosa.load(\"/home/demontego/whisper/tests/test_data/output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demontego/whisper/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "from transformers.pipelines.automatic_speech_recognition import AutomaticSpeechRecognitionPipeline\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "seconds_per_chunk = 30\n",
    "device = torch.device('cuda')\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation='sdpa',\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "generate_kwargs = {\n",
    "    'max_new_tokens': 444,\n",
    "    'num_beams': 3,\n",
    "    'condition_on_prev_tokens': False,\n",
    "    'compression_ratio_threshold': 1.35,\n",
    "    'temperature': (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "    'logprob_threshold': -1.0,\n",
    "}\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = AutomaticSpeechRecognitionPipeline(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch.float16,\n",
    "    device=device,\n",
    "    # chunk_length_s=seconds_per_chunk,  # Add chunking here\n",
    "    # batch_size=16,  # Add batch size here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demontego/whisper/.venv/lib/python3.13/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunks': [{'text': ' Субтитры создавал DimaTorzok',\n",
      "             'timestamp': (0.0, 29.98)},\n",
      "            {'text': ' Я читаю про дипсик и просто...',\n",
      "             'timestamp': (0.0, 4.0)},\n",
      "            {'text': ' Как индексели до этого не додумались, просто...',\n",
      "             'timestamp': (6.0, 10.0)},\n",
      "            {'text': ' А, нам прикольная прям архитектура, что типа на '\n",
      "                     'поверхности что-то лежало.',\n",
      "             'timestamp': (12.0, 18.0)},\n",
      "            {'text': ' Да.', 'timestamp': (19.0, 20.0)},\n",
      "            {'text': ' Там способ обучения, типа, он...',\n",
      "             'timestamp': (20.0, 23.0)},\n",
      "            {'text': '', 'timestamp': (27.0, 0.0)},\n",
      "            {'text': ' И на много раз обсуждался Но почему-то я редко видел '\n",
      "                     'его в применении',\n",
      "             'timestamp': (6.86, 13.76)},\n",
      "            {'text': ' А я знаю, почему они пошли по этому пути',\n",
      "             'timestamp': (13.76, 24.46)},\n",
      "            {'text': '', 'timestamp': (24.46, 0.0)},\n",
      "            {'text': ' Они начали обучать Давно очень',\n",
      "             'timestamp': (2.52, 3.68)},\n",
      "            {'text': ' Модель кодинга', 'timestamp': (3.68, 5.94)},\n",
      "            {'text': ' Гипсик Кодер', 'timestamp': (5.94, 7.4)},\n",
      "            {'text': ' У них была лучшая модель', 'timestamp': (7.4, 9.64)},\n",
      "            {'text': ' Вроде всех', 'timestamp': (9.64, 11.82)},\n",
      "            {'text': ' Даже лучше чем сейчас ГТТ', 'timestamp': (11.82, 14.3)},\n",
      "            {'text': ' И они такие', 'timestamp': (14.3, 16.18)},\n",
      "            {'text': ' Надо еще лучше', 'timestamp': (16.18, 17.78)},\n",
      "            {'text': ' И расширили', 'timestamp': (17.78, 19.26)},\n",
      "            {'text': ' За пределы кодинга', 'timestamp': (19.26, 23.1)},\n",
      "            {'text': ' Свою модель', 'timestamp': (23.1, 26.1)},\n",
      "            {'text': '', 'timestamp': (26.1, 0.0)},\n",
      "            {'text': ' гений считаю', 'timestamp': (5.62, 7.56)},\n",
      "            {'text': ' а че там', 'timestamp': (7.56, 9.5)},\n",
      "            {'text': ' реинфорсмент ленинг', 'timestamp': (9.5, 12.56)},\n",
      "            {'text': ' как-то используется активно',\n",
      "             'timestamp': (12.56, 14.82)},\n",
      "            {'text': ' или в чем там', 'timestamp': (14.82, 15.74)},\n",
      "            {'text': ' да там РЛ активно используется',\n",
      "             'timestamp': (15.74, 19.12)},\n",
      "            {'text': ' и там', 'timestamp': (19.12, 21.14)},\n",
      "            {'text': ' в 4 этапа', 'timestamp': (21.14, 22.88)},\n",
      "            {'text': ' обучения идет', 'timestamp': (22.88, 24.2)},\n",
      "            {'text': ' но РЛ', 'timestamp': (24.2, 27.12)},\n",
      "            {'text': ' там решает', 'timestamp': (27.12, 28.7)},\n",
      "            {'text': '', 'timestamp': (28.7, 0.0)},\n",
      "            {'text': ' И второй этап Он реальный', 'timestamp': (8.24, 9.74)},\n",
      "            {'text': ' И типа', 'timestamp': (9.74, 11.24)},\n",
      "            {'text': ' Нет модели вознаграждения', 'timestamp': (11.24, 14.74)},\n",
      "            {'text': ' Есть только правильный', 'timestamp': (14.74, 18.24)},\n",
      "            {'text': ' Неправильный ответ', 'timestamp': (18.24, 19.64)},\n",
      "            {'text': ' Вот', 'timestamp': (19.64, 21.16)},\n",
      "            {'text': ' Типа всякие математические задачи',\n",
      "             'timestamp': (21.16, 24.74)},\n",
      "            {'text': '', 'timestamp': (24.74, 0.0)},\n",
      "            {'text': ' Логические задачи Что-то по химии, по биологии, по '\n",
      "                     'физике',\n",
      "             'timestamp': (6.0, 10.9)},\n",
      "            {'text': ' Вот что-то такое', 'timestamp': (10.9, 12.26)},\n",
      "            {'text': ' Это из науки', 'timestamp': (12.26, 15.86)},\n",
      "            {'text': ' Это же полулофа, они на местной стадии',\n",
      "             'timestamp': (15.86, 24.82)},\n",
      "            {'text': ' Правильный ответ', 'timestamp': (24.82, 26.44)},\n",
      "            {'text': '', 'timestamp': (26.44, 0.0)},\n",
      "            {'text': ' Ну да, на финальной стадии у них типа Нет, на финальной '\n",
      "                     'стадии у них есть типа',\n",
      "             'timestamp': (5.28, 8.12)},\n",
      "            {'text': ' Ответ от модели', 'timestamp': (8.12, 11.42)},\n",
      "            {'text': ' И его сравнивают с истинным',\n",
      "             'timestamp': (11.42, 13.98)},\n",
      "            {'text': ' Насколько он близок', 'timestamp': (13.98, 15.78)},\n",
      "            {'text': ' Вот', 'timestamp': (15.78, 17.82)},\n",
      "            {'text': ' И тот ответ, который самый близкий',\n",
      "             'timestamp': (17.82, 22.42)},\n",
      "            {'text': ' По формуле расписывается', 'timestamp': (22.42, 24.64)},\n",
      "            {'text': ' Чтобы он был наиболее вероятным',\n",
      "             'timestamp': (24.64, 28.0)},\n",
      "            {'text': '', 'timestamp': (28.0, 0.0)},\n",
      "            {'text': ' И дальше пропускают гранилины.',\n",
      "             'timestamp': (3.52, 10.2)},\n",
      "            {'text': ' А, то есть уже потом обучают, типа, лос, и далее уже '\n",
      "                     'дают вкус, потеря.',\n",
      "             'timestamp': (18.56, 19.46)},\n",
      "            {'text': ' Ну да.', 'timestamp': (19.92, 26.72)},\n",
      "            {'text': ' ну конечно вы в этих всех темах и пока оценить качество '\n",
      "                     'да насколько ответ',\n",
      "             'timestamp': (0.0, 14.2)}],\n",
      " 'text': ' Субтитры создавал DimaTorzok Я читаю про дипсик и просто... Как '\n",
      "         'индексели до этого не додумались, просто... А, нам прикольная прям '\n",
      "         'архитектура, что типа на поверхности что-то лежало. Да. Там способ '\n",
      "         'обучения, типа, он... И на много раз обсуждался Но почему-то я редко '\n",
      "         'видел его в применении А я знаю, почему они пошли по этому пути Они '\n",
      "         'начали обучать Давно очень Модель кодинга Гипсик Кодер У них была '\n",
      "         'лучшая модель Вроде всех Даже лучше чем сейчас ГТТ И они такие Надо '\n",
      "         'еще лучше И расширили За пределы кодинга Свою модель гений считаю а '\n",
      "         'че там реинфорсмент ленинг как-то используется активно или в чем там '\n",
      "         'да там РЛ активно используется и там в 4 этапа обучения идет но РЛ '\n",
      "         'там решает И второй этап Он реальный И типа Нет модели '\n",
      "         'вознаграждения Есть только правильный Неправильный ответ Вот Типа '\n",
      "         'всякие математические задачи Логические задачи Что-то по химии, по '\n",
      "         'биологии, по физике Вот что-то такое Это из науки Это же полулофа, '\n",
      "         'они на местной стадии Правильный ответ Ну да, на финальной стадии у '\n",
      "         'них типа Нет, на финальной стадии у них есть типа Ответ от модели И '\n",
      "         'его сравнивают с истинным Насколько он близок Вот И тот ответ, '\n",
      "         'который самый близкий По формуле расписывается Чтобы он был наиболее '\n",
      "         'вероятным И дальше пропускают гранилины. А, то есть уже потом '\n",
      "         'обучают, типа, лос, и далее уже дают вкус, потеря. Ну да. ну конечно '\n",
      "         'вы в этих всех темах и пока оценить качество да насколько ответ'}\n"
     ]
    }
   ],
   "source": [
    "result = pipe(w, generate_kwargs=generate_kwargs, return_timestamps=True)\n",
    "from pprint import pprint\n",
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
